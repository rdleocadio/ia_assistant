require "ruby_llm"

class LlmService
  def self.chat(user_message:, previous_messages: [], system_prompt: default_system_prompt)
    client = RubyLLM::Client.new(
      provider: :openai,
      api_key: ENV["OPENAI_API_KEY"],
      model: "gpt-4.1-mini"
    )

    messages = []

    # system prompt = persona + contexto + regras
    messages << { role: "system", content: system_prompt }

    # histÃ³rico do chat (se vocÃª estiver salvando no banco)
    previous_messages.each do |msg|
      messages << { role: msg.role, content: msg.content }
    end

    # mensagem atual do usuÃ¡rio
    messages << { role: "user", content: user_message }

    response = client.chat(messages: messages)

    # dependendo da gem, pode ser response["choices"]...

    response["choices"].first["message"]["content"]
  rescue => e
    Rails.logger.error("LLM ERROR: #{e.message}")
    "Ops, tive um problema para responder agora. Tenta de novo em alguns segundos ðŸ™‚"
  end

  def self.default_system_prompt
    <<~PROMPT
      VocÃª Ã© uma assistente de organizaÃ§Ã£o de rotinas chamada RoutineCare.
      Seu papel Ã© ajudar o usuÃ¡rio a organizar o dia em blocos de tempo, equilibrando:

      - Tarefas obrigatÃ³rias (trabalho, estudos, compromissos fixos)
      - Autocuidado (sono, alimentaÃ§Ã£o, pausas, lazer)
      - Casa e famÃ­lia (limpeza, organizaÃ§Ã£o, cuidados com filhos/pets)

      Responda sempre em portuguÃªs claro, em formato de lista organizada por horÃ¡rios.
      Se o usuÃ¡rio nÃ£o der muitas informaÃ§Ãµes, faÃ§a perguntas rÃ¡pidas de clarificaÃ§Ã£o.
    PROMPT
  end
end
